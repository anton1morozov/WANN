{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c digit-recognizer\n",
    "# !mkdir data\n",
    "# !unzip digit-recognizer.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from wann import WANN\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join, exists\n",
    "from os import mkdir, listdir\n",
    "from typing import List\n",
    "from copy import copy\n",
    "from random import uniform\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-helicopter",
   "metadata": {},
   "source": [
    "Let's take a look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "y_train = train_df.pop('label')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show raw dataframe\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion: pandas dataframe to numpy array\n",
    "X_train = np.array(train_df).astype('float') / 255\n",
    "y_train = y_train.values\n",
    "X_test = np.array(test_df).astype('float') / 255\n",
    "columns = list(train_df.columns)\n",
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-ground",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-northern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-fisher",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wann = WANN(sensor_nodes=list(X_train.columns),\n",
    "#             output_nodes=[str(i) for i in range(10)],\n",
    "#             output_activation='softmax').init_randomly()\n",
    "# print(json.dumps(wann.to_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wann.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-cable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_series = X_train.iloc[7000]\n",
    "# img = np.reshape(img_series.values, (28, 28))\n",
    "# print(f\"np.max(img) = {np.max(img)}\")\n",
    "# print(f\"np.min(img) = {np.min(img)}\")\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wann.build_tf_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = wann.run(weight=2, sensor_data_d=img_series.to_dict(), as_numpy_array=True)\n",
    "# np.argmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-doctor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_batches(data, batchsize):\n",
    "    for i in range(0, len(data), batchsize):\n",
    "        yield data[i:i+batchsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(wanns: List[WANN], directory: str):\n",
    "    if not exists(directory):\n",
    "        mkdir(directory)\n",
    "    for i, wann in enumerate(wanns):\n",
    "        with open(join(directory, f\"WANN_{i}.json\"), 'w') as f:\n",
    "            json.dump(wann.to_dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(directory: str):\n",
    "    assert exists(directory), f\"Directory '{directory}' doesn't exist\"\n",
    "    wanns = []\n",
    "    for filename in listdir(directory):\n",
    "        with open(join(directory, filename)) as f:\n",
    "            wanns.append(WANN(input_dict=json.load(f)))\n",
    "    return wanns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pympler.tracker import SummaryTracker\n",
    "\n",
    "# tracker = SummaryTracker()\n",
    "\n",
    "# Training\n",
    "num_wanns = 10\n",
    "weights = [-2, -1, -0.5, 0.5, 1, 2]\n",
    "iterations = 1000\n",
    "sensor_nodes = list(columns)\n",
    "output_nodes = [str(i) for i in range(10)]\n",
    "batchsize = 10500\n",
    "models_dir = './models'\n",
    "\n",
    "# Initializing first wanns\n",
    "wanns = [WANN(sensor_nodes, output_nodes, output_activation='softmax').init_randomly() for _ in range(num_wanns)]\n",
    "# wanns = load_checkpoint(models_dir)\n",
    "\n",
    "# X_train = X_train[:50]\n",
    "# y_train = y_train[:50]\n",
    "\n",
    "# data = list(zip(X_train, y_train))\n",
    "y_train_onehot = np.zeros((len(y_train), 10), dtype=np.float32)\n",
    "y_train_onehot[:, y_train] = 1.0\n",
    "\n",
    "# Training\n",
    "for iteration in range(iterations):\n",
    "    \n",
    "    print(f\"Iteration {iteration}\")\n",
    "    loss = [0.] * num_wanns\n",
    "    accuracy = [0.] * num_wanns\n",
    "    \n",
    "    checkpoint(wanns, models_dir)\n",
    "    \n",
    "    for i, wann in enumerate(wanns):\n",
    "        \n",
    "        wann.build_tf_graph()\n",
    "        \n",
    "        print(f\"\\tEvaluating WANN {i}:\")\n",
    "        print(f\"\\t\\tComplexity: {wann.complexity}\")\n",
    "        \n",
    "        # Batch walking\n",
    "#         for batch in walk_batches(data, batchsize):\n",
    "#             X, y = zip(*batch)\n",
    "        for idx in range(0, len(X_train), batchsize):\n",
    "        \n",
    "            X = X_train[idx:idx+batchsize]\n",
    "            y = y_train[idx:idx+batchsize]\n",
    "            y_onehot = y_train_onehot[idx:idx+batchsize]\n",
    "            \n",
    "            # One-hot encoding label\n",
    "            onehot_labels = np.zeros((len(y), 10), dtype=np.float32)\n",
    "            onehot_labels[:, y] = 1.0\n",
    "            \n",
    "            for weight in weights:\n",
    "                results = wann.run(X, weight)\n",
    "                loss[i] += np.sum(np.linalg.norm(onehot_labels - results, axis=1))\n",
    "                accuracy[i] += np.sum(y == np.argmax(results, axis=1))\n",
    "                \n",
    "        loss[i] /= len(X_train) * len(weights)\n",
    "        accuracy[i] /= len(X_train) * len(weights)\n",
    "        print(f\"\\t\\tLoss: {loss[i]}\")\n",
    "        print(f\"\\t\\tAccuracy: {accuracy[i]}\")\n",
    "        \n",
    "        \n",
    "    # Creating list of evaluated WANNs\n",
    "    complexity = [wann.complexity for wann in wanns]\n",
    "    evaluated_wanns = list(zip(wanns, loss, accuracy, complexity))\n",
    "\n",
    "    # Choosing WANNs by mean performance\n",
    "    evaluated_wanns.sort(key=lambda p: p[1])\n",
    "\n",
    "    # Cutting WANNs with big loss (7 WANNs are left)\n",
    "    del evaluated_wanns[-2:]\n",
    "\n",
    "    # Choosing evaluation method\n",
    "    evaluation_method = \"accuracy\" if 0 <= uniform(0, 1) < 0.9 else \"complexity\"\n",
    "\n",
    "    # Evaluating remaining WANNs according to evaluation method\n",
    "    if evaluation_method == \"complexity\":\n",
    "\n",
    "        # Sorting WANNs by increasing complexity\n",
    "        evaluated_wanns.sort(key=lambda p: p[3])\n",
    "\n",
    "    elif evaluation_method == \"accuracy\":\n",
    "\n",
    "        # Sorting WANNs by decreasing accuracy\n",
    "        evaluated_wanns.sort(reverse=True, key=lambda p: p[1])\n",
    "\n",
    "    # Cutting WANNs with small maximum performance of with big complexity\n",
    "    del evaluated_wanns[-2:]\n",
    "\n",
    "    print(f\"debug: default graph len before reset: {len([n.name for n in tf.get_default_graph().as_graph_def().node])}\")\n",
    "    tf.reset_default_graph()\n",
    "    print(f\"debug: default graph len after reset: {len([n.name for n in tf.get_default_graph().as_graph_def().node])}\")\n",
    "\n",
    "    # Creating new WANNs by mutating and filling new list with them\n",
    "    new_wanns = []\n",
    "    for i in range(len(evaluated_wanns)):\n",
    "\n",
    "        # Creating mutations while its not unique\n",
    "        mutated_wann = copy(evaluated_wanns[i][0]).mutate()\n",
    "        while mutated_wann in new_wanns or mutated_wann in wanns:\n",
    "            mutated_wann = copy(evaluated_wanns[i][0]).mutate()\n",
    "        new_wanns.append(mutated_wann)\n",
    "\n",
    "        if i < 4:  # First WANNs have right to mutate twice   ODD CHECKING MIGHT BE ADD\n",
    "\n",
    "            # Creating mutations while its not unique\n",
    "            mutated_wann = copy(evaluated_wanns[i][0]).mutate()\n",
    "            while mutated_wann in new_wanns or mutated_wann in wanns:\n",
    "                mutated_wann = copy(evaluated_wanns[i][0]).mutate()\n",
    "            new_wanns.append(mutated_wann)\n",
    "\n",
    "    # Deleting evaluated WANNs\n",
    "    for ewann, _, _, _ in evaluated_wanns:\n",
    "        ewann.clear_tf_graph()\n",
    "    del evaluated_wanns\n",
    "\n",
    "    # Deleting all self.wanns\n",
    "    del wanns\n",
    "    wanns = new_wanns\n",
    "    \n",
    "#     print(\"CURRENT DEFAULT GRAPH\".center(50, '-'))\n",
    "#     print([n.name for n in tf.get_default_graph().as_graph_def().node], sep='\\n')\n",
    "    \n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     tracker.print_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-sydney",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
